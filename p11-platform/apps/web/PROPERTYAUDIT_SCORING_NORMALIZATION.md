# PropertyAudit Scoring & Recommendations Normalization

## Overview

PropertyAudit tracks GEO performance across multiple LLM surfaces (OpenAI, Claude). This document explains how scores and recommendations are normalized and aggregated.

---

## ðŸŽ¯ **Scoring Normalization**

### **Per-Surface Scores (Individual)**

Each GEO run produces a surface-specific score:

**Formula:** `LLM_SERP_SCORE = 45% Position + 25% Link + 20% SOV + 10% Accuracy`

**Components:**
1. **Position (45%):** LLM rank position (1st = 100%, 10th = 10%)
2. **Link (25%):** Citation link rank position
3. **SOV (20%):** Share of Voice (brand citations / total)
4. **Accuracy (10%):** Penalized by warning flags

**Example:**
- **OpenAI Run:** Score 75.5, Visibility 100%, Avg Rank 1.0
- **Claude Run:** Score 82.8, Visibility 100%, Avg Rank 1.0

---

### **Combined Score (Dashboard Display)**

**Aggregation Method:** Simple average of latest runs per surface

**Code:** `score/route.ts` lines 114-115
```typescript
const avgOverallScore = scores.reduce((sum, s) => sum + s.overallScore, 0) / scores.length
const avgVisibilityPct = scores.reduce((sum, s) => sum + s.visibilityPct, 0) / scores.length
```

**Example:**
- OpenAI: 75.5
- Claude: 82.8
- **Combined: 79.2** (displayed on dashboard)

**Rationale:** Equal weighting assumes OpenAI and Claude have similar user adoption.

---

## ðŸ“Š **Recommendations Normalization**

### **Data Collection Strategy**

**Current Implementation:**
- Fetches last **2 runs** (ideally 1 OpenAI + 1 Claude)
- Changed from 5 runs to ensure recent, paired comparison
- Groups answers by query across both surfaces

**Code:** `recommendation-engine.ts` line 149
```typescript
runsQuery = runsQuery.limit(2)  // Get latest paired runs
```

---

### **Per-Query Analysis (Model-Aware)**

For each query, recommendations now track:

```typescript
modelBreakdown: {
  openai: {
    presence: boolean,
    rank: number | null,
    sov: number | null
  },
  claude: {
    presence: boolean,
    rank: number | null,
    sov: number | null
  },
  affectedModels: ['openai', 'claude']  // Which models have the issue
}
```

**Example Scenario:**

**Query:** "Best apartments in San Diego"
- **OpenAI:** Present, Rank #1, SOV 20%
- **Claude:** Absent, Rank null, SOV null
- **Recommendation:** "No presence - Affects Claude only"

---

### **Presence Detection (Optimistic)**

**Rule:** If **ANY** model shows presence â†’ query has presence

**Code:** `identifyMissingKeywords` line 218
```typescript
const hasPresence = answers.some(a => a.presence)
```

**Rationale:**
- If OpenAI shows you â†’ you're visible to OpenAI users
- If Claude doesn't â†’ opportunity to improve Claude performance
- Recommendation shows model-specific action needed

**Impact:** Fewer "missing keyword" recommendations (optimistic view)

---

### **Rank Improvements (Model-Specific)**

**Rule:** If rank > 3 on **ANY** model â†’ generate recommendation

**Enhancement:** Recommendation shows which model needs improvement

**Example:**
- **Title:** "Ranking #5 for 'luxury apartments'"
- **Description:** "Currently averaging #5. Issue primarily on OPENAI."
- **Model Breakdown:**
  - OpenAI: Rank #5 âš ï¸
  - Claude: Rank #1 âœ“

**Actionable:** User knows to focus content optimization for OpenAI specifically.

---

### **Competitor Analysis (Cross-Model)**

**Rule:** Aggregates competitor mentions across **BOTH** models

**Code:** `fetchAnalysisContext` lines 163-188
```typescript
answers?.forEach((answer: any) => {
  answer.ordered_entities.forEach((entity: any) => {
    competitorMap.get(key)!.mentions.push(entity.position)
  })
})
```

**Rationale:**
- Competitors appearing on either model are threats
- Aggregate view shows overall competitive landscape
- Individual recommendations show per-model gaps

---

## ðŸŽ¨ **UI Display Enhancements**

### **Recommendation Cards Now Show:**

1. **Model Performance Box** (NEW)
   - OpenAI: âœ“ Present / âœ— Absent + Rank
   - Claude: âœ“ Present / âœ— Absent + Rank
   - Affects: OPENAI, CLAUDE

2. **Enhanced Descriptions**
   - "Issue affects OPENAI only"
   - "Issue affects both OpenAI and Claude"
   - "Issue primarily on CLAUDE"

3. **Actionable Intelligence**
   - Users know exactly which model needs work
   - Can prioritize based on their audience's LLM preferences

---

## ðŸ”„ **Scoring Flow Diagram**

```
Individual Runs:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenAI Run  â”‚         â”‚ Claude Run  â”‚
â”‚ 22 queries  â”‚         â”‚ 22 queries  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                       â”‚
       â”‚ Evaluate              â”‚ Evaluate
       â”‚ per query             â”‚ per query
       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Score: 75.5 â”‚         â”‚ Score: 82.8 â”‚
â”‚ Vis: 100%   â”‚         â”‚ Vis: 100%   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ Average
                   â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Combined:   â”‚
            â”‚ Score: 79.2 â”‚  â† Dashboard display
            â”‚ Vis: 100%   â”‚
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ Analyze per query
                   â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Recommendations:    â”‚
            â”‚ - Model breakdowns  â”‚  â† Per-model insights
            â”‚ - Affected models   â”‚
            â”‚ - Targeted actions  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“ˆ **Impact Score Normalization**

**Impact scores (0-100) are standardized across all recommendation types:**

| Type | High Priority | Medium Priority | Low Priority |
|------|--------------|-----------------|--------------|
| Missing Branded | 90 | - | - |
| Missing Category | - | 70 | - |
| Missing Other | - | - | 50 |
| Rank Improvement (4-7) | 85 | - | - |
| Content Gap (rank 4-5) | - | 60-40 | - |
| Citation Opportunity | - | 40-75 | - |
| Voice Search | - | - | 55 |
| Maintenance | - | - | 40-50 |

**Priority Weighting:**
- High: 3x weight
- Medium: 2x weight  
- Low: 1x weight

---

## âœ… **Summary: What's Normalized?**

| Metric | Normalization Method | Rationale |
|--------|---------------------|-----------|
| **Overall Score** | âœ“ Averaged across models | Equal weight to OpenAI & Claude |
| **Visibility %** | âœ“ Averaged across models | Shows combined visibility |
| **Avg LLM Rank** | âœ“ Averaged across models | Overall ranking performance |
| **Recommendations** | âœ“ Model-aware analysis | Shows which model needs work |
| **Impact Scores** | âœ“ Standardized 0-100 | Comparable across rec types |
| **Priority Levels** | âœ“ Consistent formula | Branded > Category > Other |

---

## ðŸŽ¯ **Key Benefits of Model-Aware Analysis**

1. **Targeted Optimization**
   - Know which LLM needs improvement
   - Focus efforts where needed most

2. **Competitive Intelligence**
   - See if competitors dominate specific models
   - Adjust strategy per platform

3. **Better ROI**
   - Don't optimize where you're already #1
   - Address actual gaps, not phantom issues

4. **Trend Analysis**
   - Track model-specific performance over time
   - Identify if one model is degrading

---

## ðŸš€ **Future Enhancements**

### **Weighted Averages by Usage**
If you have data on which LLM your audience uses:
```typescript
const weightedScore = 
  (openaiScore * 0.6) +   // 60% of users use OpenAI
  (claudeScore * 0.4)     // 40% of users use Claude
```

### **Model-Specific Thresholds**
Different targets per model:
```typescript
{
  openai: { targetVisibility: 80%, targetRank: 2 },
  claude: { targetVisibility: 70%, targetRank: 3 }
}
```

### **A/B Testing Queries**
Test query effectiveness per model:
```typescript
// Which queries perform better on OpenAI vs Claude?
// Optimize query wording per model
```

---

## Conclusion

âœ… **Scores:** Fully normalized via simple averaging  
âœ… **Recommendations:** Model-aware with per-model breakdowns  
âœ… **UI:** Shows exactly which model needs work  
âœ… **Impact:** Standardized for comparability  

This provides **actionable, model-specific intelligence** while maintaining simple aggregate views for quick assessment.
